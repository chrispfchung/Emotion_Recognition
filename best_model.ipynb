{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries & model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "import cv2\n",
    "from resizeimage import resizeimage\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model into notebook\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model_weights.h5\")\n",
    "print(\"Loaded model into notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "                     optimizer=\"sgd\",\n",
    "                     metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 1, 1, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 455       \n",
      "=================================================================\n",
      "Total params: 20,098,759\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 74,375\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions dictionary\n",
    "emotions = {\"anger\" : 0,\n",
    "\"disgust\" : 1,\n",
    "\"fear\" : 2,\n",
    "\"happy\" : 3,\n",
    "\"sad\" : 4,\n",
    "\"surprise\" : 5,\n",
    "\"neutral\" : 6}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/opencv/opencv/tree/master/data/haarcascades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch Video w/ Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "# Get user supplied values\n",
    "# imagePath = sys.argv[1]\n",
    "# cascPath = sys.argv[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the opencv file to detect face\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Conver to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Face Detection\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x,y,w,h) in faces:\n",
    "        crop_img = gray[y:y+h, x:x+w]\n",
    "\n",
    "        # Get width and height\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        # Resize for our model (48x48x1)\n",
    "        small = cv2.resize(crop_img, dsize = (48,48))\n",
    "        # convert size from 48x48 to 1x48x48\n",
    "        image3D = np.expand_dims(small,axis = 0)\n",
    "        # convert to 1x48x48x1\n",
    "        image4D = np.expand_dims(image3D, axis = 3)\n",
    "        # convert to 1x48x48x3\n",
    "        image4D3 = np.repeat(image4D, 3, axis=3)\n",
    "\n",
    "        # Model each frame\n",
    "        emotions_prob = loaded_model.predict(image4D3)[0]\n",
    "        # Convert emotion probabilities into binary, where 1 is the emotion you're feeling\n",
    "        listt = [1 if metric == emotions_prob.max() else 0 for metric in emotions_prob]\n",
    "        # Get the index 1 in the binary list, listt \n",
    "        emotion_index = listt.index(1)\n",
    "        emotion = list(emotions.keys())[emotion_index]\n",
    "\n",
    "        # Show Emotion on Video\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        text_placement  = (int(width/2 - 500),int(height/2 + 100))\n",
    "        fontScale = 1\n",
    "        fontColor = (255,255,255)\n",
    "        lineType = 4\n",
    "\n",
    "        cv2.putText(frame, \n",
    "            '{}'.format(emotion), \n",
    "            text_placement, \n",
    "            font, \n",
    "            fontScale,\n",
    "            fontColor,\n",
    "            lineType)\n",
    "    \n",
    "        # Display the resulting frame\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(20) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
